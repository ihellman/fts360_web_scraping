library(tidyverse)
library(RSelenium)
library(wdman)
# Start a docker container with chrome.  Docker makes this whole process more platform agnostic.
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
Sys.sleep(5)
# List of iridium pages on FTS360.  Used to iterate,
stationdf <- tribble(
~stationID, ~siteurl,
"BN", "https://360.ftsinc.com/218/station/6959",
"FW", "https://360.ftsinc.com/218/station/6688",
"TE", "https://360.ftsinc.com/218/station/6650",
"TW", "https://360.ftsinc.com/218/station/6960") %>%
data.frame()
# CCreate connection to chrome running in docker container.
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost",
port = 4445L,
browserName = "chrome")
# some VERY secure entry of usernames
username <- "uienrep@gmail.com"
pass <- "ftsenrep"
# go to page, enter login and pw, and click login
remDr$open()
Sys.sleep(5)
remDr$navigate("https://360.ftsinc.com/login")
Sys.sleep(5)
remDr$findElement(using = "xpath", value = "//*[(@id = 'email')]")$sendKeysToElement(list(username))
remDr$findElement(using = "xpath", value = '//*[(@id = "password")]')$sendKeysToElement(list(pass))
remDr$findElements("xpath", '//*[(@id = "login-button")]')[[1]]$clickElement()
Sys.sleep(5)
# Navigate to each site's url, scrape data from latest data table, and clean the data
scrapefts <- function(stationID, siteurl){
#navgiate to page.
remDr$navigate(siteurl)
Sys.sleep(5)
# find table and download data to readable format
tableElem <- remDr$findElement(using = "xpath", '//*[contains(concat( " ", @class, " " ), concat( " ", "grid-page", " " ))]')
tableElem$getElementAttribute("class")
tableVec <- tableElem$getElementText()[[1]]
#Extract date and clean it up a bit.
dateTimeExtracted <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
str_subset(",") %>%
pluck(1) %>%
str_remove(pattern = " \\s*\\([^\\)]+\\)")
# Clean and convert vector to data frame.
dat <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
matrix(ncol = 3, byrow = TRUE) %>%
.[-1,2] %>%
parse_number() %>%
t() %>%
data.frame() %>%
rename("voltage_V" = "X1", "h2Temp_C" = "X2", "stage_ft" = "X3", "LSU" = "X4") %>%
add_column(datetimeUTC = dateTimeExtracted, .before = TRUE,
stationID = stationID) %>%
mutate(datetimeUTC = mdy_hms(datetimeUTC)) %>%
mutate(datetimeUTC = as.character(datetimeUTC)) %>%
relocate(stationID, .before = datetimeUTC)
print(dat)
return(dat)
}
# scrape data for all stations listed in station dataframe.
new_data <- pmap_df(stationdf, scrapefts)
# Navigate to each site's url, scrape data from latest data table, and clean the data
scrapefts <- function(stationID, siteurl){
#navgiate to page.
remDr$navigate(siteurl)
Sys.sleep(5)
# find table and download data to readable format
tableElem <- remDr$findElement(using = "xpath", '//*[contains(concat( " ", @class, " " ), concat( " ", "grid-page", " " ))]')
tableElem$getElementAttribute("class")
tableVec <- tableElem$getElementText()[[1]]
#Extract date and clean it up a bit.
dateTimeExtracted <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
str_subset(",") %>%
pluck(1) %>%
str_remove(pattern = " \\s*\\([^\\)]+\\)")
# Clean and convert vector to data frame.
dat <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
matrix(ncol = 3, byrow = TRUE) %>%
.[-1,2] %>%
parse_number() %>%
t() %>%
data.frame() %>%
rename("voltage_V" = "X1", "h2Temp_C" = "X2", "stage_ft" = "X3", "LSU" = "X4") %>%
add_column(datetimeUTC = dateTimeExtracted, .before = TRUE,
stationID = stationID) %>%
#mutate(datetimeUTC = mdy_hms(datetimeUTC)) %>%
mutate(datetimeUTC = as.character(datetimeUTC)) %>%
relocate(stationID, .before = datetimeUTC)
print(dat)
return(dat)
}
# scrape data for all stations listed in station dataframe.
new_data <- pmap_df(stationdf, scrapefts)
# Navigate to each site's url, scrape data from latest data table, and clean the data
scrapefts <- function(stationID, siteurl){
#navgiate to page.
remDr$navigate(siteurl)
Sys.sleep(5)
# find table and download data to readable format
tableElem <- remDr$findElement(using = "xpath", '//*[contains(concat( " ", @class, " " ), concat( " ", "grid-page", " " ))]')
tableElem$getElementAttribute("class")
tableVec <- tableElem$getElementText()[[1]]
#Extract date and clean it up a bit.
dateTimeExtracted <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
str_subset(",") %>%
pluck(1) %>%
str_remove(pattern = " \\s*\\([^\\)]+\\)")
# Clean and convert vector to data frame.
dat <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
matrix(ncol = 3, byrow = TRUE) %>%
.[-1,2] %>%
parse_number() %>%
t() %>%
data.frame() %>%
rename("voltage_V" = "X1", "h2Temp_C" = "X2", "stage_ft" = "X3", "LSU" = "X4") %>%
add_column(datetimeUTC = dateTimeExtracted, .before = TRUE,
stationID = stationID) %>%
mutate(datetimeUTC = lubridate::mdy_hms(datetimeUTC)) %>%
mutate(datetimeUTC = as.character(datetimeUTC)) %>%
relocate(stationID, .before = datetimeUTC)
print(dat)
return(dat)
}
# scrape data for all stations listed in station dataframe.
new_data <- pmap_df(stationdf, scrapefts)
str(new_data)
# read In existing data
existing_data <- read_csv(dataFileLocation) %>%
mutate(datetimeUTC = as.character(datetimeUTC))
dataFileLocation <- "/Users/ianhellman/Documents/Github/fts360_web_scraping/secrets/iridium_sed_event_data.csv"
# read In existing data
existing_data <- read_csv(dataFileLocation) %>%
mutate(datetimeUTC = as.character(datetimeUTC))
existing_data
merged_data <- bind_rows(existing_data, new_data) %>%
distinct() %>%
mutate(datetimeUTC = as.character(datetimeUTC))
merged_data
new_data <- tibble(new_data)
new_data
existing_data
merged_data <- bind_rows(existing_data, new_data) %>%
distinct() %>%
mutate(datetimeUTC = as.character(datetimeUTC))
merged_data
bind_rows(existing_data, new_data)
bind_rows(existing_data, new_data) %>%
distinct()
?distinct
merged_data[1,]
merged_data[2,]
merged_data[2,] == merged_data[2,]
merged_data[2,] == merged_data[5,]
merged_data[2,5] == merged_data[5,5]
merged_data[2,5]
merged_data[5,5]
a <- merged_data[5,5]
a
b <- merged_data[2,5]
a == b
a
b
a
b
pull(a)
a<-pull(a)
b<-pull(b)
a == b
a
b
str(a)
str(b)
1 == 1
c <- 23.4
c
merged_data
merged_data %>% mutate(stage_ft = as.numeric(stage_ft))
merged_data %>% mutate(stage_ft = as.numeric(stage_ft)) %>% distinct()
merged_data %>% mutate(stage_ft = as.character(stage_ft)) %>% distinct()
merged_data %>% mutate(stage_ft = as.double(stage_ft)) %>% distinct()
merged_data %>% mutate(stage_ft = as.character(stage_ft)) %>% distinct()
merged_data <- bind_rows(existing_data, new_data) %>%
mutate(datetimeUTC = as.character(datetimeUTC),
stage_ft = as.character(stage_ft)) %>%
distinct()
merged_data
# Navigate to each site's url, scrape data from latest data table, and clean the data
scrapefts <- function(stationID, siteurl){
#navgiate to page.
remDr$navigate(siteurl)
Sys.sleep(5)
# find table and download data to readable format
tableElem <- remDr$findElement(using = "xpath", '//*[contains(concat( " ", @class, " " ), concat( " ", "grid-page", " " ))]')
tableElem$getElementAttribute("class")
tableVec <- tableElem$getElementText()[[1]]
#Extract date and clean it up a bit.
dateTimeExtracted <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
str_subset(",") %>%
pluck(1) %>%
str_remove(pattern = " \\s*\\([^\\)]+\\)")
# Clean and convert vector to data frame.
dat <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
matrix(ncol = 3, byrow = TRUE) %>%
.[-1,2] %>%
parse_number() %>%
t() %>%
data.frame() %>%
rename("voltage_V" = "X1", "h2Temp_C" = "X2", "stage_ft" = "X3", "LSU" = "X4") %>%
add_column(datetimeUTC = dateTimeExtracted, .before = TRUE,
stationID = stationID) %>%
mutate(datetimeUTC = lubridate::mdy_hms(datetimeUTC)) %>%
mutate(datetimeUTC = as.character(datetimeUTC)) %>%
relocate(stationID, .before = datetimeUTC)
print(dat)
return(dat)
}
# scrape data for all stations listed in station dataframe.
new_data <- pmap_df(stationdf, scrapefts)
# Navigate to each site's url, scrape data from latest data table, and clean the data
scrapefts <- function(stationID, siteurl){
#navgiate to page.
remDr$navigate(siteurl)
Sys.sleep(5)
# find table and download data to readable format
tableElem <- remDr$findElement(using = "xpath", '//*[contains(concat( " ", @class, " " ), concat( " ", "grid-page", " " ))]')
tableElem$getElementAttribute("class")
tableVec <- tableElem$getElementText()[[1]]
#Extract date and clean it up a bit.
dateTimeExtracted <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
str_subset(",") %>%
pluck(1) %>%
str_remove(pattern = " \\s*\\([^\\)]+\\)")
# Clean and convert vector to data frame.
dat <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
matrix(ncol = 3, byrow = TRUE) %>%
.[-1,2] %>%
parse_number() %>%
t() %>%
data.frame() %>%
rename("voltage_V" = "X1", "h2Temp_C" = "X2", "stage_ft" = "X3", "LSU" = "X4") %>%
add_column(datetimeUTC = dateTimeExtracted, .before = TRUE,
stationID = stationID) %>%
mutate(datetimeUTC = lubridate::mdy_hms(datetimeUTC)) %>%
mutate(datetimeUTC = as.character(datetimeUTC)) %>%
relocate(stationID, .before = datetimeUTC)
print(dat)
return(dat)
}
# scrape data for all stations listed in station dataframe.
new_data <- pmap_df(stationdf, scrapefts)
# close browser sesssions.
remDr$close()
system("docker kill $(docker ps -q)")
library(tidyverse)
library(RSelenium)
library(wdman)
# Start a docker container with chrome.  Docker makes this whole process more platform agnostic.
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
Sys.sleep(5)
# List of iridium pages on FTS360.  Used to iterate,
stationdf <- tribble(
~stationID, ~siteurl,
"BN", "https://360.ftsinc.com/218/station/6959",
"FW", "https://360.ftsinc.com/218/station/6688",
"TE", "https://360.ftsinc.com/218/station/6650",
"TW", "https://360.ftsinc.com/218/station/6960") %>%
data.frame()
# CCreate connection to chrome running in docker container.
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost",
port = 4445L,
browserName = "chrome")
# some VERY secure entry of usernames
username <- "uienrep@gmail.com"
pass <- "ftsenrep"
# go to page, enter login and pw, and click login
remDr$open()
Sys.sleep(5)
remDr$navigate("https://360.ftsinc.com/login")
Sys.sleep(5)
remDr$findElement(using = "xpath", value = "//*[(@id = 'email')]")$sendKeysToElement(list(username))
remDr$findElement(using = "xpath", value = '//*[(@id = "password")]')$sendKeysToElement(list(pass))
remDr$findElements("xpath", '//*[(@id = "login-button")]')[[1]]$clickElement()
Sys.sleep(5)
# Navigate to each site's url, scrape data from latest data table, and clean the data
scrapefts <- function(stationID, siteurl){
#navgiate to page.
remDr$navigate(siteurl)
Sys.sleep(5)
# find table and download data to readable format
tableElem <- remDr$findElement(using = "xpath", '//*[contains(concat( " ", @class, " " ), concat( " ", "grid-page", " " ))]')
tableElem$getElementAttribute("class")
tableVec <- tableElem$getElementText()[[1]]
#Extract date and clean it up a bit.
dateTimeExtracted <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
str_subset(",") %>%
pluck(1) %>%
str_remove(pattern = " \\s*\\([^\\)]+\\)")
# Clean and convert vector to data frame.
dat <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
matrix(ncol = 3, byrow = TRUE) %>%
.[-1,2] %>%
parse_number() %>%
t() %>%
data.frame() %>%
rename("voltage_V" = "X1", "h2Temp_C" = "X2", "stage_ft" = "X3", "LSU" = "X4") %>%
add_column(datetimeUTC = dateTimeExtracted, .before = TRUE,
stationID = stationID) %>%
mutate(datetimeUTC = lubridate::mdy_hms(datetimeUTC)) %>%
mutate(datetimeUTC = as.character(datetimeUTC)) %>%
relocate(stationID, .before = datetimeUTC)
print(dat)
return(dat)
}
# scrape data for all stations listed in station dataframe.
new_data <- pmap_df(stationdf, scrapefts)
dataFileLocation <- "/Users/ianhellman/Documents/Github/fts360_web_scraping/secrets/iridium_sed_event_data.csv"
write.csv(dat, dataFileLocation, row.names = FALSE)
write.csv(new_data, dataFileLocation, row.names = FALSE)
if (file.exists(dataFileLocation)){
# read In existing data
existing_data <- read_csv(dataFileLocation) %>%
mutate(datetimeUTC = as.character(datetimeUTC))
merged_data <- bind_rows(existing_data, new_data) %>%
mutate(datetimeUTC = as.character(datetimeUTC),
stage_ft = as.character(stage_ft)) %>%
distinct()
}
merged_data
new_data
if (file.exists(dataFileLocation)){
# read In existing data
existing_data <- read_csv(dataFileLocation) %>%
mutate(datetimeUTC = as.character(datetimeUTC))
merged_data <- bind_rows(existing_data, new_data) %>%
mutate(datetimeUTC = as.character(datetimeUTC),
stage_ft = as.character(stage_ft)) %>%
distinct()
} else {
write.csv(new_data, dataFileLocation, row.names = FALSE)
}
# kill docker container process (not sure if this is best or if it should just be left running)
system("docker kill $(docker ps -q)")
# Ian Hellman
# 2022-02-03
#
# This script extracts Iridium telemetry data from the FTS360 website by  navigating
# to each basin's page and copying the table of current values.
library(tidyverse)
library(RSelenium)
library(wdman)
# Start a docker container with chrome.  Docker makes this whole process more platform agnostic.
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
Sys.sleep(5)
### Actual scraping -----------------------------------------------------------------------------------------------
#Connect to server -> Find Iridium data table -> repeat for all sites -> merge to one table
# List of iridium pages on FTS360.  Used to iterate,
stationdf <- tribble(
~stationID, ~siteurl,
"BN", "https://360.ftsinc.com/218/station/6959",
"FW", "https://360.ftsinc.com/218/station/6688",
"TE", "https://360.ftsinc.com/218/station/6650",
"TW", "https://360.ftsinc.com/218/station/6960") %>%
data.frame()
# Create connection to chrome running in docker container.
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost",
port = 4445L,
browserName = "chrome")
# some VERY secure entry of usernames
username <- "uienrep@gmail.com"
pass <- "ftsenrep"
# Go to login page
remDr$open()
Sys.sleep(5)
remDr$navigate("https://360.ftsinc.com/login")
Sys.sleep(5)
# Enter login info and press login button
# Email
remDr$findElement(using = "xpath", value = "//*[(@id = 'email')]")$sendKeysToElement(list(username))
# Password
remDr$findElement(using = "xpath", value = '//*[(@id = "password")]')$sendKeysToElement(list(pass))
# Login button
remDr$findElements("xpath", '//*[(@id = "login-button")]')[[1]]$clickElement()
Sys.sleep(5)
# Navigate to each site's url, scrape data from latest data table, and clean the data
scrapefts <- function(stationID, siteurl){
#navgiate to page.
remDr$navigate(siteurl)
Sys.sleep(5)
# find table and download data to readable format
tableElem <- remDr$findElement(using = "xpath", '//*[contains(concat( " ", @class, " " ), concat( " ", "grid-page", " " ))]')
tableElem$getElementAttribute("class")
tableVec <- tableElem$getElementText()[[1]]
#Extract date and clean it up a bit.
dateTimeExtracted <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
str_subset(",") %>%
pluck(1) %>%
str_remove(pattern = " \\s*\\([^\\)]+\\)")
# Clean and convert vector to data frame.
dat <- str_split(tableVec, pattern = "\n") %>%
unlist() %>%
matrix(ncol = 3, byrow = TRUE) %>%
.[-1,2] %>%
parse_number() %>%
t() %>%
data.frame() %>%
rename("voltage_V" = "X1", "h2Temp_C" = "X2", "stage_ft" = "X3", "LSU" = "X4") %>%
add_column(datetimeUTC = dateTimeExtracted, .before = TRUE,
stationID = stationID) %>%
mutate(datetimeUTC = lubridate::mdy_hms(datetimeUTC)) %>%
mutate(datetimeUTC = as.character(datetimeUTC)) %>%
relocate(stationID, .before = datetimeUTC)
print(dat)
return(dat)
}
# scrape data for all stations listed in station dataframe.
new_data <- pmap_df(stationdf, scrapefts)
###  Merge with existing data or export new --------------------------------------------------------------------
# Location of stored data (either existing or to be saved)
dataFileLocation <- "/Users/ianhellman/Documents/Github/fts360_web_scraping/data/iridium_sed_event_data.csv"
# Logic where: if data file does not exist, create one.  If it does exist then import it and
# merge with newly downloaded data.
if (file.exists(dataFileLocation)){
# read In existing data
existing_data <- read_csv(dataFileLocation) %>%
mutate(datetimeUTC = as.character(datetimeUTC))
# new_data and existing data somehow have different datatypes which seems impossible
# given they are in same table.  hack fix is as.character below.  very weird.
merged_data <- bind_rows(existing_data, new_data) %>%
mutate(datetimeUTC = as.character(datetimeUTC),
stage_ft = as.character(stage_ft)) %>%
distinct()
} else {
write.csv(new_data, dataFileLocation, row.names = FALSE)
}
### Clean up --------------------------------------------------------------------------------------------------
#close browser sesssions.
remDr$close()
# kill docker container process (not sure if this is best or if it should just be left running)
system("docker kill $(docker ps -q)")
new_data
file.exists(dataFileLocation)
